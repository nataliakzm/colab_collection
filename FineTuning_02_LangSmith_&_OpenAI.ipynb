{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliakzm/colab_collection/blob/main/FineTuning_02_LangSmith_%26_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangSmith + OpenAI Fine-tuning Guide\n",
        "\n",
        "## Context\n",
        "\n",
        "This is a guide for fine-tuning OpenAI's `gpt-3.5-turbo` model on an example knowledge-extraction task.\n",
        "\n",
        "## Environment\n",
        "\n",
        "Fist we'll set our `LANGCHAIN_API_KEY` so that we can access our LangSmith datasets as well as an `OPENAI_API_KEY` for fine-tuning and inference."
      ],
      "metadata": {
        "id": "wM8MRkf8Dr94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env LANGCHAIN_API_KEY=\n",
        "%env OPENAI_API_KEY="
      ],
      "metadata": {
        "id": "b3zo3LXZgHNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet -U langchain\n",
        "%pip install --quiet -U langsmith\n",
        "%pip install --quiet -U openai"
      ],
      "metadata": {
        "id": "XNNAe_PvgK-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get dataset\n",
        "\n",
        "We are loading a private dataset from LangSmith (e.g., in this case `Carb-IE-train`). We will conver this to an openai compatible format.\n"
      ],
      "metadata": {
        "id": "A-8dt5qqtpgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langsmith\n",
        "import json\n",
        "\n",
        "client = langsmith.Client()\n",
        "\n",
        "def craft_messages(input, output) -> list[dict]:\n",
        "    out = json.dumps(output[\"clusters\"])\n",
        "    return [{\"role\": \"user\", \"content\": \"Extract triplets from the following sentence:\\n\\n\" + input[\"sentence\"]},\n",
        "            {\"role\": \"assistant\", \"content\": out}]\n"
      ],
      "metadata": {
        "id": "laL_xs4MH6DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "data = [\n",
        "    craft_messages(example.inputs, example.outputs) for example in itertools.islice(client.list_examples(dataset_name=\"Carb-IE-train\"), 50)\n",
        "    ]"
      ],
      "metadata": {
        "id": "qC_-a9GnI8Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Upload training file to OpenAI"
      ],
      "metadata": {
        "id": "rYV3Q_BGyx9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from io import BytesIO\n",
        "\n",
        "my_file = BytesIO()\n",
        "for m in data:\n",
        "    my_file.write((json.dumps({\"messages\": m}) + \"\\n\").encode('utf-8'))\n",
        "\n",
        "my_file.seek(0)\n",
        "training_file = openai.File.create(\n",
        "  file=my_file,\n",
        "  purpose='fine-tune'\n",
        ")"
      ],
      "metadata": {
        "id": "98mTrCMaI8lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Begin training"
      ],
      "metadata": {
        "id": "hyHE-b0SzEiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job = openai.FineTuningJob.create(training_file=training_file.id, model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "C7HeoOjhwVm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wait for training to complete"
      ],
      "metadata": {
        "id": "k5c0xn_UzIf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "while True:\n",
        "  ftj = openai.FineTuningJob.retrieve(job.id)\n",
        "  if ftj.fine_tuned_model is None:\n",
        "    print(f\"Waiting for fine-tuning to complete... Elapsed: {time.time() - start}\", end=\"\\r\", flush=True)\n",
        "    time.sleep(10)\n",
        "  else:\n",
        "    print(ftj.fine_tuned_model, flush=True)\n",
        "    break"
      ],
      "metadata": {
        "id": "5034kMSrO7H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Construct fine-tuned chain"
      ],
      "metadata": {
        "id": "kQVdV07YzMh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import prompts\n",
        "from langchain import chat_models\n",
        "\n",
        "prompt = prompts.ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "      (\"human\", \"Extract triplets from the following sentence:\\n\\n{sentence}\"),\n",
        "    ]\n",
        ")\n",
        "llm = chat_models.ChatOpenAI(model=ftj.fine_tuned_model, temperature=0)\n",
        "finetuned_chain = prompt | llm"
      ],
      "metadata": {
        "id": "7JMqmaLPO_yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate\n",
        "\n",
        "We'll make a custom evaluator for triplets."
      ],
      "metadata": {
        "id": "ktIMK0G-zP6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import smith\n",
        "import json\n",
        "from typing import Any, Optional\n",
        "from langchain.evaluation import StringEvaluator\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import openai_functions\n",
        "\n",
        "eval_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an impartial grader tasked with measuring the accuracy of extracted entity relations.\"),\n",
        "        (\"human\", \"Please evaluate the following data:\\n\\n\"\n",
        "         \"<INPUT>\\n{input}</INPUT>\\n\"\n",
        "         \"<PREDICTED>\\n{prediction}</PREDICTED>\\n\"\n",
        "         \"<GROUND_TRUTH>\\n{reference}</GROUND_TRUTH>\\n\\n\"\n",
        "         \"Please save your reasoning and grading by calling the commit_grade function.\"\n",
        "         \" First, enumerate all factual discrepancies in the predicted triplets relative to the ground truth.\"\n",
        "         \" Finally, score the prediction on a scale out of 100, taking into account factuality and\"\n",
        "         \" correctness according to the ground truth.\"),\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "commit_grade_schema = {\n",
        "    \"name\": \"commit_grade\",\n",
        "    \"description\": \"Commits a grade with reasoning.\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"commit_grade_parameters\",\n",
        "        \"description\": \"Parameters for the commit_grade function.\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"mistakes\": {\n",
        "                \"title\": \"discrepancies\",\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Any discrepencies between the predicted and ground truth.\"\n",
        "            },\n",
        "            \"reasoning\": {\n",
        "                \"title\": \"reasoning\",\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The explanation or logic behind the final grade.\"\n",
        "            },\n",
        "            \"grade\": {\n",
        "                \"title\": \"grade\",\n",
        "                \"type\": \"number\",\n",
        "                \"description\": \"The numerical value representing the grade.\",\n",
        "                \"minimum\": 0,\n",
        "                \"maximum\": 100\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"reasoning\", \"grade\", \"mistakes\"],\n",
        "    }\n",
        "}\n",
        "\n",
        "def normalize_grade(func_args: str) -> dict:\n",
        "    args = json.loads(func_args)\n",
        "    return {\n",
        "        \"reasoning\": (args.get(\"reasoning\", \"\") + \"\\n\\n\" + args.get(\"discrepancies\", \"\")).strip(),\n",
        "        \"score\": args.get(\"grade\", 0) / 100,\n",
        "    }\n",
        "\n",
        "eval_chain = (\n",
        "    eval_prompt\n",
        "    | ChatOpenAI(model=\"gpt-4\", temperature=0).bind(functions=[commit_grade_schema])\n",
        "    | openai_functions.OutputFunctionsParser()\n",
        "    | normalize_grade\n",
        ")\n",
        "\n",
        "class EvaluateTriplets(StringEvaluator):\n",
        "    \"\"\"Evaluate the triplets of a predicted string.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        *,\n",
        "        prediction: str,\n",
        "        reference: Optional[str] = None,\n",
        "        input: Optional[str] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> dict:\n",
        "        callbacks = kwargs.pop(\"callbacks\", None)\n",
        "        return eval_chain.invoke(\n",
        "            {\"prediction\": prediction, \"reference\": reference, \"input\": input},\n",
        "            {\"callbacks\": callbacks},\n",
        "        )\n",
        "\n",
        "config = smith.RunEvalConfig(\n",
        "    custom_evaluators=[EvaluateTriplets()],\n",
        ")"
      ],
      "metadata": {
        "id": "2wYX2jgxPYNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset_name = \"Carb-IE-test\"\n",
        "results = await client.arun_on_dataset(validation_dataset_name, finetuned_chain, evaluation=config)"
      ],
      "metadata": {
        "id": "HodqFYZaPrPz",
        "outputId": "608c2e82-e925-4aec-a5e1-5835013d8011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project '6f1ba87705c54527bf5bbc4668938260-RunnableSequence' at:\n",
            "https://smith.langchain.com/projects/p/6ab2ed7a-ca7f-44f4-913e-ce10459f7a79?eval=true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /runs/8b9a0160-1fcb-429a-9438-293f7903c774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare against few-shot variants\n",
        "\n",
        "Here, we benchmark the fine-tuned model against gpt-3.5 and gpt-4 chains prompted with 5 examples."
      ],
      "metadata": {
        "id": "G989Z-6dzf_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_example\n",
        "\n",
        "from langchain import prompts\n",
        "from langchain import chat_models\n",
        "\n",
        "first_5 = list(itertools.islice(client.list_examples(dataset_name=\"Carb-IE-train\"), 5))\n",
        "messages = []\n",
        "partials = {}\n",
        "for i in range(len(first_5)):\n",
        "  messages.extend([\n",
        "        (\"human\", \"Extract triplets from the following sentence:\\n\\n{input_%d}\" % i),\n",
        "        (\"ai\", \"{output_%d}\" % i)\n",
        "    ]\n",
        "  )\n",
        "  partials[\"input_%d\" % i] = first_5[i].inputs[\"sentence\"]\n",
        "  partials[\"output_%d\" % i] = json.dumps(first_5[i].outputs[\"clusters\"])\n",
        "\n",
        "messages.append((\"human\", \"Extract triplets from the following sentence:\\n\\n{sentence}\"))\n",
        "\n",
        "prompt = prompts.ChatPromptTemplate.from_messages(\n",
        "    messages\n",
        ").partial(\n",
        "    **partials\n",
        ")\n",
        "llm = chat_models.ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "fewshot_chain = prompt | llm"
      ],
      "metadata": {
        "id": "PACbcpu-P6nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset_name = \"Carb-IE-test\"\n",
        "results = await client.arun_on_dataset(validation_dataset_name, fewshot_chain, evaluation=config, project_name=\"Few-shot-GPT-3.5\")"
      ],
      "metadata": {
        "id": "euYyBH-NT2T3",
        "outputId": "8e4da8ec-edc2-414a-828e-8654871cb303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'Few-shot-GPT-3.5' at:\n",
            "https://smith.langchain.com/projects/p/34798b10-cfed-471b-89f3-1743725993d8?eval=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fewshot_gpt4_chain = prompt | chat_models.ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "validation_dataset_name = \"Carb-IE-test\"\n",
        "results = await client.arun_on_dataset(validation_dataset_name, fewshot_gpt4_chain, evaluation=config, project_name=\"Few-shot-GPT-4\")"
      ],
      "metadata": {
        "id": "N0exEBAeUAlB",
        "outputId": "b0830288-42f4-4563-bb96-6f7f21dd18c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'Few-shot-GPT-4' at:\n",
            "https://smith.langchain.com/projects/p/80ae7670-56df-470c-8c57-ccf86678a728?eval=true\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}